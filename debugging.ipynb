{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import PIL.Image\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=input(\"place your system prompt\")),\n",
    "    HumanMessage(\n",
    "        content=input(\"place your prompt here:\")\n",
    "    ),\n",
    "    # SystemMessage(content=input(\"place your system prompt\")),\n",
    "    # HumanMessage(\n",
    "    #     content=[\n",
    "    #         {\"type\": \"text\", \"text\": input(\"place your user prompt\")},\n",
    "    #         {\"type\": \"image_url\", \"image_url\": input(\"place your image url\")},\n",
    "    #     ]\n",
    "    # ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=[], messages=[SystemMessage(content='you are a javascript master class developper'), HumanMessage(content='write me bubble sort algorithm')])\n"
     ]
    }
   ],
   "source": [
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_messages = prompt.format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='you are a javascript master class developper'),\n",
      " HumanMessage(content='write me bubble sort algorithm')]\n"
     ]
    }
   ],
   "source": [
    "pprint(formatted_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Below is the implementation of the Bubble Sort algorithm in JavaScript. This sorting technique repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.\n",
      "\n",
      "```javascript\n",
      "function bubbleSort(arr) {\n",
      "    let n = arr.length;\n",
      "    \n",
      "    // Outer loop: Traverse through all array elements\n",
      "    for (let i = 0; i < n-1; i++) {\n",
      "        // Inner loop: Compare adjacent elements and swap if needed\n",
      "        for (let j = 0; j < n-i-1; j++) {\n",
      "            // Swap elements if the element found is greater than the next element\n",
      "            if (arr[j] > arr[j+1]) {\n",
      "                let temp = arr[j];\n",
      "                arr[j] = arr[j + 1];\n",
      "                arr[j + 1] = temp;\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    \n",
      "    return arr; // Return the sorted array\n",
      "}\n",
      "\n",
      "// Example usage:\n",
      "const unsortedArray = [64, 34, 25, 12, 22, 11, 90];\n",
      "console.log(\"Unsorted Array:\", unsortedArray);\n",
      "const sortedArray = bubbleSort(unsortedArray);\n",
      "console.log(\"Sorted Array (Bubble Sort):\", sortedArray);\n",
      "```\n",
      "\n",
      "This function will sort an array of numbers in ascending order using the Bubble Sort algorithm. While this implementation is straightforward and easy to understand, it's worth noting that bubble sort has a time complexity of O(n^2), which makes it less efficient for large datasets compared to more advanced sorting algorithms like quicksort or mergesort. However, it serves as an excellent starting point to grasp the basic principles of sorting mechanisms in JavaScript."
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"phi3\")\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "function bubbleSort(arr) {\n",
    "    let n = arr.length;\n",
    "    \n",
    "    // Outer loop: Traverse through all array elements\n",
    "    for (let i = 0; i < n-1; i++) {\n",
    "        // Inner loop: Compare adjacent elements and swap if needed\n",
    "        for (let j = 0; j < n-i-1; j++) {\n",
    "            // Swap elements if the element found is greater than the next element\n",
    "            if (arr[j] > arr[j+1]) {\n",
    "                let temp = arr[j];\n",
    "                arr[j] = arr[j + 1];\n",
    "                arr[j + 1] = temp;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return arr; // Return the sorted array\n",
    "}\n",
    "\n",
    "// Example usage:\n",
    "const unsortedArray = [64, 34, 25, 12, 22, 11, 90];\n",
    "console.log(\"Unsorted Array:\", unsortedArray);\n",
    "const sortedArray = bubbleSort(unsortedArray);\n",
    "console.log(\"Sorted Array (Bubble Sort):\", sortedArray);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with LLMChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a world class javascript developer , you take a concept and write an implementation about it\"),\n",
    "    (\"human\", \"CONCEPT:{concept}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a world class javascript developer , you take a concept and write an implementation about it\n",
      "Human: CONCEPT:djikstra algorithm\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " The Dijkstra's Algorithm is a popular graph traversal method used to find the shortest path between two nodes in a weighted, directed or undirected graph. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and later published in 1969. In this implementation, we will implement it using JavaScript with priority queue (min heap) for efficient processing of nodes.\n",
      "\n",
      "Here's a step-by-step breakdown of the algorithm:\n",
      "\n",
      "1. Create an empty set to store visited nodes.\n",
      "2. Initialize all distances from the source node as Infinity, except for itself which is 0.\n",
      "3. Use priority queue (min heap) and insert the source node with distance 0.\n",
      "4. While the priority queue is not empty:\n",
      "   a. Extract the current node with the smallest distance (use PriorityQueue's pop method).\n",
      "   b. Mark it as visited by adding it to the visited set.\n",
      "   c. Iterate through its neighbors and update their distances if possible.\n",
      "5. Return the shortest path from source to destination or an indication that no such path exists.\n",
      "\n",
      "Now, let's implement Dijkstra's Algorithm in JavaScript:\n",
      "\n",
      "```javascript\n",
      "class PriorityQueue {\n",
      "  constructor() {\n",
      "    this.items = [];\n",
      "  }\n",
      "\n",
      "  enqueue(element, priority) {\n",
      "    const queueElement = { element, priority };\n",
      "    let contain = false;\n",
      "\n",
      "    for (let i = 0; i < this.items.length; i++) {\n",
      "      if (this.items[i].priority > queueElement.priority) {\n",
      "        this.items.splice(i++, 0, queueElement);\n",
      "        contain = true;\n",
      "        break;\n",
      "      }\n",
      "    }\n",
      "\n",
      "    if (!contain) {\n",
      "      this.items.push(queueElement);\n",
      "    const newLength = this.items.length;\n",
      "    while (newLength--) {\n",
      "      console.log(this.items);\n",
      "    }\n",
      "    return true;\n",
      "  }}\n",
      "\n",
      "  dequeue() {\n",
      "    if (this.isEmpty()) return 'Underflow';\n",
      "    return this.items.shift();\n",
      "  }\n",
      "\n",
      "  isEmpty() {\n",
      "    return this.items.length === 0;\n",
      "  }\n",
      "}\n",
      "\n",
      "function dijkstra(graph, source, destination) {\n",
      "  let distances = {};\n",
      "  const visitedNodes = new Set();\n",
      "  const priorityQueue = new PriorityQueue();\n",
      "\n",
      "  // Initialize all distances from the source node as Infinity\n",
      "  for (let node in graph.nodes) {\n",
      "    if (node === source) {\n",
      "      distances[node] = 0;\n",
      "      priorityQueue.enqueue(node, 0);\n",
      "    } else {\n",
      "      distances[node] = Infinity;\n",
      "    }\n",
      "  }\n",
      "\n",
      "  while (!priorityQueue.isEmpty()) {\n",
      "    let currentNode = priorityQueue.dequeue().element;\n",
      "\n",
      "    if (currentNode === destination) break; // Destination reached, no need to continue\n",
      "\n",
      "    visitedNodes.add(currentNode);\n",
      "\n",
      "    for (let neighbor in graph.nodes[currentNode].neighbors) {\n",
      "      let nextNeighbor = graph.nodes[currentNode].neighbors[neighbor];\n",
      "      \n",
      "      if (!visitedNodes.has(nextNeighbor)) { // If not visited, continue processing\n",
      "        const tentativeDistance = distances[currentNode] + nextNeighbor.weight;\n",
      "\n",
      "        if (tentativeDistance < distances[nextNeighbor]) {\n",
      "          distances[nextNeighbor] = tentativeDistance;\n",
      "          priorityQueue.enqueue(nextNeighbor, tentativeDistance);\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "\n",
      "  return destination in distances ? `The shortest path from ${source} to ${destination} is of length: ${distances[destination]}` : 'No path found';\n",
      "}\n",
      "\n",
      "// Example usage\n",
      "const graph = {\n",
      "  nodes: {\n",
      "    1: { neighbors: {2: {weight: 7}, 3: {weight: 9}}, distance: Infinity },\n",
      "    2: { neighbors: {4: {weight: 10}}, distance: Infinity },\n",
      "    3: { neighbors: {}, distance: Infinity },\n",
      "    4: { neighbors: {}, distance: Infinity }\n",
      "  },\n",
      "  source: 1,\n",
      "  destination: 4\n",
      "};\n",
      "\n",
      "console Point(x, y) {\n",
      "    this.x = x;\n",
      "    this.y = y;\n",
      "}\n",
      "\n",
      "const points = [new Point(0, 0), new Point(1, 1), new Point(2, 2)];\n",
      "\n",
      "// Define the distances between nodes\n",
      "const edges = {\n",
      "  1: { 2: 7 },\n",
      "  2: { 4: 10 },\n",
      "};\n",
      "\n",
      "console.log(dijkstra(graph, graph.source, graph.destination)); // Outputs: The shortest path from 1 to 4 is of length: 17\n",
      "```\n",
      "\n",
      "In this implementation, we used a simple Point class for Node objects and defined an example graph with weighted edges using adjacent node points as keys in the `graph.nodes` object. This implementation can be easily adapted to your specific data structures or use cases."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"concept\": input(\"explain your concept\")}):\n",
    "    print(chunk[\"text\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unsupervised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\routes.py\", line 732, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1924, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1646, in preprocess_data\n",
      "    inputs_cached = await processing_utils.async_move_files_to_cache(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 494, in async_move_files_to_cache\n",
      "    return await client_utils.async_traverse(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1008, in async_traverse\n",
      "    new_obj.append(await async_traverse(item, func, is_root))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1008, in async_traverse\n",
      "    new_obj.append(await async_traverse(item, func, is_root))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 1003, in async_traverse\n",
      "    new_obj[key] = await async_traverse(value, func, is_root)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio_client\\utils.py\", line 999, in async_traverse\n",
      "    return await func(json_obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py\", line 465, in _move_to_cache\n",
      "    raise ValueError(\n",
      "ValueError: File C:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\human is not in the upload folder and cannot be accessed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "verbose_output = \"\"\n",
    "\n",
    "TEXT2TEXT = \"Text-to-Text\"\n",
    "IMAGE2TEXT = \"Image-to-Text\"\n",
    "OLLAMA = \"Ollama\"\n",
    "GOOGLE = \"Google Gemini\"\n",
    "\n",
    "\n",
    "def user(message:str, history:List[List])->tuple:\n",
    "    return message , history + [[[\"human\" ,message], None]]\n",
    "\n",
    "\n",
    "def bot(\n",
    "    model:str, temperature:float ,history: List[List[BaseMessage] | List[List[str]]], api_key:str=\"\"\n",
    "):\n",
    "    history[-1][1] = [\"ai\", \"\"]  # type: ignore\n",
    "    if \"gemini\" in model.lower() and \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        llm = ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key) # type: ignore\n",
    "    elif \"GOOGLE_API_KEY\" in os.environ:\n",
    "        llm = ChatGoogleGenerativeAI(model=model, temperature=temperature) # type: ignore\n",
    "    elif \"gemini\" not in model.lower():\n",
    "        llm = Ollama(model=model)\n",
    "\n",
    "    langchain_history_messages = []\n",
    "    for chat_message in history:\n",
    "        for human , ai in chat_message:\n",
    "            langchain_history_messages.append(HumanMessage(content=human[1]))\n",
    "            langchain_history_messages.append(AIMessage(content=ai[1]))\n",
    "    \n",
    "    match model:\n",
    "        case \"gemini-pro\":\n",
    "            for chunk in llm.stream(langchain_history_messages):\n",
    "                history[-1][1][-1] += chunk[\"output\"] # type: ignore\n",
    "                yield history\n",
    "        case \"phi3\":\n",
    "            for chunk in llm.stream(langchain_history_messages):\n",
    "                history[-1][1][-1] += chunk # type: ignore\n",
    "                yield history\n",
    "        case _:\n",
    "            yield \"no such a model exist\"\n",
    "\n",
    "def main():\n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Base(\n",
    "            primary_hue=\"teal\",\n",
    "            secondary_hue=\"cyan\",\n",
    "            neutral_hue=\"slate\",\n",
    "        )\n",
    "    ) as main_app:\n",
    "        gr.Markdown(\"# Multimodal LLM\")\n",
    "        with gr.Row():\n",
    "            ## inputs\n",
    "            with gr.Column():\n",
    "                with gr.Tab(label=\"Text Workflows\"):\n",
    "                    message = gr.TextArea(\n",
    "                        label=\"User Prompt\",\n",
    "                        placeholder=\"Place your prompt here\",\n",
    "                        container=False,\n",
    "                        show_label=False,\n",
    "                    )\n",
    "                with gr.Accordion(label=\"Avanced Configs\"):\n",
    "                    temperature = gr.Slider(maximum=1, step=0.1)\n",
    "                    model = gr.Dropdown(choices=[\"gemini-pro\", \"phi3\"], value=\"phi3\", label=\"Models\")\n",
    "\n",
    "                    @gr.render(inputs=model)\n",
    "                    def display_api_key_input(chosen_model: str):\n",
    "                        if \"gemini\" in chosen_model.lower():\n",
    "                            api_key = gr.Textbox(\n",
    "                                \"\",\n",
    "                                label=\"Google API Key\",\n",
    "                                placeholder=\"place your api key here\",\n",
    "                            )\n",
    "\n",
    "                with gr.Row():\n",
    "                    generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                    clear_btn = gr.ClearButton(variant=\"stop\")       \n",
    "\n",
    "\n",
    "            ## bot output\n",
    "            with gr.Column():\n",
    "                chatbot = gr.Chatbot(label=\"Bot Response\")\n",
    "\n",
    "        generate_btn.click(\n",
    "            user, [message, chatbot],[message, chatbot]\n",
    "        ).then(\n",
    "            bot, [model, temperature, chatbot], [chatbot], queue=False\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            lambda:None, None, chatbot, queue=False\n",
    "        )\n",
    "\n",
    "    main_app.queue()\n",
    "    main_app.launch(debug=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample error handling v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "import gradio as gr\n",
    "import logging\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=\"app.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s %(levelname)s:%(message)s\",\n",
    ")\n",
    "\n",
    "logging.info(\"Starting the script\")\n",
    "\n",
    "try:\n",
    "    load_dotenv(find_dotenv(), override=True)\n",
    "    logging.info(\"Loaded environment variables\")\n",
    "except Exception as e:\n",
    "    logging.error(\"Error loading environment variables: %s\", e)\n",
    "\n",
    "verbose_output = \"\"\n",
    "\n",
    "TEXT2TEXT = \"Text-to-Text\"\n",
    "IMAGE2TEXT = \"Image-to-Text\"\n",
    "OLLAMA = \"Ollama\"\n",
    "GOOGLE = \"Google Gemini\"\n",
    "\n",
    "\n",
    "def user(message: str, history: List[List]) -> tuple:\n",
    "    logging.info(\"User function called with message: %s\", message)\n",
    "    return message, history + [[[\"human\", message], None]]\n",
    "\n",
    "\n",
    "def bot(\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    history: List[List[BaseMessage] | List[List[str]]],\n",
    "    api_key: str = \"\",\n",
    "):\n",
    "    logging.info(\n",
    "        \"Bot function called with model: %s, temperature: %s\", model, temperature\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        history[-1][1] = [\"ai\", \"\"]  # type: ignore\n",
    "        if \"gemini\" in model.lower() and \"GOOGLE_API_KEY\" not in os.environ:\n",
    "            llm = ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)  # type: ignore\n",
    "        elif \"GOOGLE_API_KEY\" in os.environ:\n",
    "            llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)  # type: ignore\n",
    "        elif \"gemini\" not in model.lower():\n",
    "            llm = Ollama(model=model)\n",
    "        logging.info(\"LLM initialized\")\n",
    "\n",
    "        langchain_history_messages = []\n",
    "        for chat_message in history:\n",
    "            for human, ai in chat_message:\n",
    "                langchain_history_messages.append(HumanMessage(content=human[1]))\n",
    "                langchain_history_messages.append(AIMessage(content=ai[1]))\n",
    "\n",
    "        match model:\n",
    "            case \"gemini-pro\":\n",
    "                for chunk in llm.stream(langchain_history_messages):\n",
    "                    history[-1][1][-1] += chunk[\"output\"]  # type: ignore\n",
    "                    yield history\n",
    "            case \"phi3\":\n",
    "                for chunk in llm.stream(langchain_history_messages):\n",
    "                    history[-1][1][-1] += chunk  # type: ignore\n",
    "                    yield history\n",
    "            case _:\n",
    "                yield \"no such a model exist\"\n",
    "                logging.warning(\"Model not found: %s\", model)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in bot function: %s\", e)\n",
    "        yield \"An error occurred, please check the logs for more details.\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Entering main function\")\n",
    "\n",
    "    try:\n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Base(\n",
    "                primary_hue=\"teal\",\n",
    "                secondary_hue=\"cyan\",\n",
    "                neutral_hue=\"slate\",\n",
    "            )\n",
    "        ) as main_app:\n",
    "            logging.info(\"Initialized Gradio Blocks\")\n",
    "\n",
    "            gr.Markdown(\"# Multimodal LLM\")\n",
    "            with gr.Row():\n",
    "                ## inputs\n",
    "                with gr.Column():\n",
    "                    with gr.Tab(label=\"Text Workflows\"):\n",
    "                        message = gr.TextArea(\n",
    "                            label=\"User Prompt\",\n",
    "                            placeholder=\"Place your prompt here\",\n",
    "                            container=False,\n",
    "                            show_label=False,\n",
    "                        )\n",
    "                        logging.info(\"Created TextArea for user prompt\")\n",
    "\n",
    "                    with gr.Accordion(label=\"Advanced Configs\"):\n",
    "                        temperature = gr.Slider(maximum=1, step=0.1)\n",
    "                        model = gr.Dropdown(\n",
    "                            choices=[\"gemini-pro\", \"phi3\"], value=\"phi3\", label=\"Models\"\n",
    "                        )\n",
    "                        logging.info(\"Created Slider and Dropdown for configs\")\n",
    "\n",
    "                        @gr.render(inputs=model)\n",
    "                        def display_api_key_input(chosen_model: str):\n",
    "                            logging.info(\n",
    "                                \"Inside display_api_key_input function with model: %s\",\n",
    "                                chosen_model,\n",
    "                            )\n",
    "                            if \"gemini\" in chosen_model.lower():\n",
    "                                api_key = gr.Textbox(\n",
    "                                    \"\",\n",
    "                                    label=\"Google API Key\",\n",
    "                                    placeholder=\"place your api key here\",\n",
    "                                )\n",
    "                                logging.info(\"Created Textbox for Google API Key\")\n",
    "\n",
    "                    with gr.Row():\n",
    "                        generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                        clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "                        logging.info(\"Created Generate and Clear buttons\")\n",
    "\n",
    "                ## bot output\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(label=\"Bot Response\")\n",
    "                    logging.info(\"Created Chatbot for bot response\")\n",
    "\n",
    "        generate_btn.click(user, [message, chatbot], [message, chatbot]).then(\n",
    "            bot, [model, temperature, chatbot], [chatbot], queue=False\n",
    "        )\n",
    "        logging.info(\"Set up button click handlers\")\n",
    "\n",
    "        clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "        logging.info(\"Set up clear button handler\")\n",
    "\n",
    "        main_app.queue()\n",
    "        main_app.launch(debug=True)\n",
    "        logging.info(\"Launched Gradio app\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in the main function: %s\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Script started\")\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logging.critical(\"Fatal error in main execution: %s\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample error handling v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yahia\\Desktop\\python_basics\\python_django_projects\\langchain_bot\\gradio_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical error: render() got multiple values for argument 'inputs'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"app.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"gemini-pro\": \"google/gemini-pro\",\n",
    "    \"phi3\": \"openai/phi3\",\n",
    "}\n",
    "\n",
    "def user(message: str, chatbot: Optional[gr.Chatbot] = None) -> Tuple[str, Optional[gr.Chatbot]]:\n",
    "    logging.debug(\"User function called with message: %s\", message)\n",
    "    if chatbot is None:\n",
    "        chatbot = gr.Chatbot()\n",
    "    chatbot.append((\"User\", message))\n",
    "    return \"\", chatbot\n",
    "\n",
    "def bot(model: str, temperature: float, chatbot: gr.Chatbot) -> gr.Chatbot:\n",
    "    try:\n",
    "        logging.debug(\"Bot function called with model: %s, temperature: %f\", model, temperature)\n",
    "        # Simulate a response\n",
    "        response = f\"This is a simulated response from model {model} with temperature {temperature}.\"\n",
    "        chatbot.append((\"Bot\", response))\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in bot function: %s\", str(e))\n",
    "    return chatbot\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting the Gradio app\")\n",
    "    try:\n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Base(\n",
    "                primary_hue=\"teal\",\n",
    "                secondary_hue=\"cyan\",\n",
    "                neutral_hue=\"slate\",\n",
    "            )\n",
    "        ) as main_app:\n",
    "            gr.Markdown(\"# Multimodal LLM\")\n",
    "            with gr.Row():\n",
    "                # Inputs\n",
    "                with gr.Column():\n",
    "                    with gr.Tab(label=\"Text Workflows\"):\n",
    "                        message = gr.TextArea(\n",
    "                            label=\"User Prompt\",\n",
    "                            placeholder=\"Place your prompt here\",\n",
    "                            container=False,\n",
    "                            show_label=False,\n",
    "                        )\n",
    "                    with gr.Accordion(label=\"Advanced Configs\"):\n",
    "                        temperature = gr.Slider(maximum=1, step=0.1, label=\"Temperature\")\n",
    "                        model = gr.Dropdown(choices=[\"gemini-pro\", \"phi3\"], value=\"phi3\", label=\"Models\")\n",
    "\n",
    "                        def display_api_key_input(chosen_model: str):\n",
    "                            \"\"\"\n",
    "                            Display API key input only if the chosen model is a Gemini model.\n",
    "\n",
    "                            Args:\n",
    "                                chosen_model (str): Selected model.\n",
    "\n",
    "                            Returns:\n",
    "                                gr.Textbox: API Key input box if Gemini model is chosen.\n",
    "                            \"\"\"\n",
    "                            logging.debug(\"Rendering API key input for model: %s\", chosen_model)\n",
    "                            if \"gemini\" in chosen_model.lower():\n",
    "                                api_key = gr.Textbox(\n",
    "                                    \"\",\n",
    "                                    label=\"Google API Key\",\n",
    "                                    placeholder=\"Place your API key here\",\n",
    "                                )\n",
    "                                return api_key\n",
    "                            return None\n",
    "\n",
    "                        api_key_input = gr.render(display_api_key_input, inputs=model)\n",
    "\n",
    "                    with gr.Row():\n",
    "                        generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                        clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "\n",
    "                # Bot output\n",
    "                with gr.Column():\n",
    "                    chatbot = gr.Chatbot(label=\"Bot Response\")\n",
    "\n",
    "            generate_btn.click(\n",
    "                user, [message, chatbot], [message, chatbot]\n",
    "            ).then(\n",
    "                bot, [model, temperature, chatbot], [chatbot], queue=False\n",
    "            )\n",
    "            clear_btn.click(\n",
    "                lambda: None, None, chatbot, queue=False\n",
    "            )\n",
    "\n",
    "        main_app.queue()\n",
    "        main_app.launch(debug=True)\n",
    "    except Exception as e:\n",
    "        logging.critical(\"Critical error in main function: %s\", str(e))\n",
    "        print(f\"Critical error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        bot_message = random.choice([\"How are you?\", \"I love you\", \"I'm very hungry\"])\n",
    "        history[-1][1] = \"\"\n",
    "        for character in bot_message:\n",
    "            history[-1][1] += character\n",
    "            time.sleep(0.05)\n",
    "            yield history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latest-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system and utility imports\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "# gradio interface library imports\n",
    "import gradio as gr\n",
    "\n",
    "# langchain imports\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "# user message addition logic\n",
    "def user(message: str, history: List[List]):\n",
    "    return message, history + [[message, None]]\n",
    "\n",
    "\n",
    "# bot response logic\n",
    "def bot(\n",
    "    history: List[List[str]],\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    api_key: str = \"\",\n",
    "):\n",
    "    history[-1][-1] = \"\"  # type: ignore\n",
    "    if \"gemini\" in model.lower() and \"GOOGLE_API_KEY\" not in os.environ:\n",
    "        llm = ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)  # type: ignore\n",
    "    elif \"GOOGLE_API_KEY\" in os.environ:\n",
    "        llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)  # type: ignore\n",
    "    elif \"gemini\" not in model.lower():\n",
    "        llm = Ollama(model=model)\n",
    "\n",
    "    match model:\n",
    "        case \"gemini-pro\":\n",
    "            for chunk in llm.stream(history[-1][0]):\n",
    "                pprint(chunk)\n",
    "                history[-1][1] += chunk.content\n",
    "                yield history\n",
    "        case \"phi3\":\n",
    "            for chunk in llm.stream(history[-1][0]):\n",
    "                pprint(chunk)\n",
    "                history[-1][1] += chunk\n",
    "                yield history\n",
    "        case _:\n",
    "            yield \"not a supported model!\"\n",
    "\n",
    "\n",
    "# interface design\n",
    "def main():\n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Base(\n",
    "            primary_hue=\"teal\",\n",
    "            secondary_hue=\"cyan\",\n",
    "            neutral_hue=\"slate\",\n",
    "        )\n",
    "    ) as main_app:\n",
    "        gr.Markdown(\"# Multimodal LLM\")\n",
    "        with gr.Row():\n",
    "            ## inputs\n",
    "            with gr.Column():\n",
    "                with gr.Tab(label=\"Text Workflows\"):\n",
    "                    message = gr.TextArea(\n",
    "                        label=\"User Prompt\",\n",
    "                        placeholder=\"Place your prompt here\",\n",
    "                        container=False,\n",
    "                        show_label=False,\n",
    "                    )\n",
    "                with gr.Accordion(label=\"Avanced Configs\"):\n",
    "                    temperature = gr.Slider(maximum=1, step=0.1)\n",
    "                    model = gr.Dropdown(\n",
    "                        choices=[\"gemini-pro\", \"phi3\"],\n",
    "                        value=\"gemini-pro\",\n",
    "                        label=\"Models\",\n",
    "                    )\n",
    "\n",
    "                @gr.render(inputs=model)\n",
    "                def display_api_key_input(chosen_model: str):\n",
    "                    if \"gemini\" in chosen_model.lower():\n",
    "                        api_key = gr.Textbox(\n",
    "                            \"AIzaSyAwdxQfnpVrMaExufEk4A4JPEWl6hTyaQE\",\n",
    "                            label=\"Google API Key\",\n",
    "                            placeholder=\"place your api key here\",\n",
    "                            type=\"password\",\n",
    "                        )\n",
    "                        with gr.Row():\n",
    "                            generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                            clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "\n",
    "                        generate_btn.click(\n",
    "                            user, [message, chatbot], [message, chatbot]\n",
    "                        ).then(\n",
    "                            bot,\n",
    "                            [chatbot, model, temperature, api_key],\n",
    "                            [chatbot],\n",
    "                            queue=False,\n",
    "                        )\n",
    "                        clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                        with gr.Row():\n",
    "                            generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                            clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "\n",
    "                        generate_btn.click(\n",
    "                            user, [message, chatbot], [message, chatbot]\n",
    "                        ).then(\n",
    "                            bot, [chatbot, model, temperature], [chatbot], queue=False\n",
    "                        )\n",
    "                        clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "            ## bot output\n",
    "            with gr.Column():\n",
    "                chatbot = gr.Chatbot(label=\"Bot Response\")\n",
    "\n",
    "    # driver code\n",
    "    main_app.queue()\n",
    "    main_app.launch(debug=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system and utility imports\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "from typing import List\n",
    "\n",
    "# gradio interface library imports\n",
    "import gradio as gr\n",
    "\n",
    "# langchain imports\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Configure logging to both console and file\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler('app.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it for both handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# User message addition logic\n",
    "def user(message: str, history: List[List]):\n",
    "    logger.info(\"User function called with message: %s\", message)\n",
    "    try:\n",
    "        return message, history + [[message, None]]\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in user function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "# Bot response logic\n",
    "def bot(\n",
    "    history: List[List[str]],\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    api_key: str = \"\",\n",
    "):\n",
    "    logger.info(\"Bot function called with model: %s, temperature: %f\", model, temperature)\n",
    "    try:\n",
    "        history[-1][-1] = \"\"  # type: ignore\n",
    "        if \"gemini\" in model.lower() and \"GOOGLE_API_KEY\" not in os.environ:\n",
    "            llm = ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)  # type: ignore\n",
    "        elif \"GOOGLE_API_KEY\" in os.environ:\n",
    "            llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)  # type: ignore\n",
    "        elif \"gemini\" not in model.lower():\n",
    "            llm = Ollama(model=model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model}\")\n",
    "\n",
    "        match model:\n",
    "            case \"gemini-pro\":\n",
    "                for chunk in llm.stream(history[-1][0]):\n",
    "                    logger.info(\"Received chunk: %s\", chunk)\n",
    "                    history[-1][1] += chunk.content\n",
    "                    yield history\n",
    "            case \"phi3\":\n",
    "                for chunk in llm.stream(history[-1][0]):\n",
    "                    logger.info(\"Received chunk: %s\", chunk)\n",
    "                    history[-1][1] += chunk\n",
    "                    yield history\n",
    "            case _:\n",
    "                logger.error(\"Not a supported model: %s\", model)\n",
    "                yield \"not a supported model!\"\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in bot function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "# Interface design\n",
    "def main():\n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Base(\n",
    "            primary_hue=\"teal\",\n",
    "            secondary_hue=\"cyan\",\n",
    "            neutral_hue=\"slate\",\n",
    "        )\n",
    "    ) as main_app:\n",
    "        gr.Markdown(\"# Multimodal LLM\")\n",
    "        with gr.Row():\n",
    "            ## inputs\n",
    "            with gr.Column():\n",
    "                with gr.Tab(label=\"Text Workflows\"):\n",
    "                    message = gr.TextArea(\n",
    "                        label=\"User Prompt\",\n",
    "                        placeholder=\"Place your prompt here\",\n",
    "                        container=False,\n",
    "                        show_label=False,\n",
    "                    )\n",
    "                with gr.Accordion(label=\"Avanced Configs\"):\n",
    "                    temperature = gr.Slider(maximum=1, step=0.1)\n",
    "                    model = gr.Dropdown(\n",
    "                        choices=[\"gemini-pro\", \"phi3\"], value=\"gemini-pro\", label=\"Models\"\n",
    "                    )\n",
    "\n",
    "                @gr.render(inputs=model)\n",
    "                def display_api_key_input(chosen_model: str):\n",
    "                    try:\n",
    "                        if \"gemini\" in chosen_model.lower():\n",
    "                            api_key = gr.Textbox(\n",
    "                                \"AIzaSyAwdxQfnpVrMaExufEk4A4JPEWl6hTyaQE\",\n",
    "                                label=\"Google API Key\",\n",
    "                                placeholder=\"place your api key here\",\n",
    "                                type=\"password\",\n",
    "                            )\n",
    "                            with gr.Row():\n",
    "                                generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                                clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "\n",
    "                            generate_btn.click(\n",
    "                                user, [message, chatbot], [message, chatbot]\n",
    "                            ).then(\n",
    "                                bot,\n",
    "                                [chatbot, model, temperature, api_key],\n",
    "                                [chatbot],\n",
    "                                queue=False,\n",
    "                            )\n",
    "                            clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "                        else:\n",
    "                            with gr.Row():\n",
    "                                generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "                                clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "\n",
    "                            generate_btn.click(\n",
    "                                user, [message, chatbot], [message, chatbot]\n",
    "                            ).then(\n",
    "                                bot,\n",
    "                                [chatbot, model, temperature],\n",
    "                                [chatbot],\n",
    "                                queue=False,\n",
    "                            )\n",
    "                            clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "                    except Exception as e:\n",
    "                        logger.error(\"Error in display_api_key_input function: %s\", e)\n",
    "                        logger.debug(traceback.format_exc())\n",
    "                        raise\n",
    "            ## bot output\n",
    "            with gr.Column():\n",
    "                chatbot = gr.Chatbot(label=\"Bot Response\")\n",
    "\n",
    "    # Driver code\n",
    "    try:\n",
    "        main_app.queue()\n",
    "        main_app.launch(debug=True)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in main function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bot logic logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7900\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7900/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# system and utility imports\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "from typing import List\n",
    "# gradio interface library imports\n",
    "import gradio as gr\n",
    "\n",
    "# langchain imports\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Configure logging to both console and file\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler(\"app.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it for both handlers\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "css = \"\"\"\n",
    ".full_height {\n",
    "    height:100vh;\n",
    "} \n",
    "\"\"\"\n",
    "\n",
    "# User message addition logic\n",
    "def user(message: str, history: List[List]):\n",
    "    logger.info(\"User function called with message: %s\", message)\n",
    "    try:\n",
    "        return message, history + [[message, None]]\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in user function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# Bot response logic\n",
    "def bot(\n",
    "    history: List[List[str]],\n",
    "    model: str,\n",
    "    temperature: float,\n",
    "    api_key: str = \"\",\n",
    "    system_prompt: str = \"\"\n",
    "):\n",
    "    logger.info(\n",
    "        \"Bot function called with model: %s, temperature: %f\", model, temperature\n",
    "    )\n",
    "    try:\n",
    "        history[-1][-1] = \"\"  # type: ignore\n",
    "        match model:\n",
    "            case \"gemini-pro\":\n",
    "                if \"GOOGLE_API_KEY\" in os.environ:\n",
    "                    logger.info(\"instantiating ChatGoogleGenerativeAI\")\n",
    "                    llm = ChatGoogleGenerativeAI(model=model, temperature=temperature)  # type: ignore\n",
    "                else:\n",
    "                    logger.info(\"instantiating ChatGoogleGenerativeAI\")\n",
    "                    llm = ChatGoogleGenerativeAI(model=model, temperature=temperature, google_api_key=api_key)  # type: ignore\n",
    "            case \"phi3\":\n",
    "                logger.info(\"instantiating Ollama\")\n",
    "                llm = Ollama(model=model)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported model: {model}\")\n",
    "\n",
    "        # if model == \"gemini-pro\":\n",
    "        #     logger.info(\"Using gemini-pro model\")\n",
    "        #     for chunk in llm.stream(history[-1][0]):\n",
    "        #         logger.info(\"Received chunk: %s\", chunk)\n",
    "        #         history[-1][1] += chunk.content  # Ensure content is properly added\n",
    "        #         yield history\n",
    "        # elif model == \"phi3\":\n",
    "        #     logger.info(\"Using phi3 model\")\n",
    "        #     pprint(llm.base_url)\n",
    "        #     for chunk in llm.stream(history[-1][0]):\n",
    "        #         logger.info(\"Received chunk: %s\", chunk)\n",
    "        #         history[-1][1] += chunk\n",
    "        #         yield history\n",
    "        # else:\n",
    "        #     logger.error(\"Not a supported model: %s\", model)\n",
    "        #     yield \"not a supported model!\"\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        template = f\"\"\"\n",
    "SYSTEM PROMPT: {system_prompt}\n",
    "TASK: {}\n",
    "\"\"\"\n",
    "        prompt = PromptTemplate.from_template()\n",
    "        if model == \"gemini-pro\":\n",
    "            logger.info(\"Using gemini-pro model\")\n",
    "            response = llm.invoke(history[-1][0])\n",
    "            logger.info(\"Received response: %s\", response)\n",
    "            history[-1][1] += response.content  # type: ignore # Ensure content is properly added\n",
    "            return history\n",
    "        elif model == \"phi3\":\n",
    "            logger.info(\"Using phi3 model\")\n",
    "            response = llm.invoke(history[-1][0])\n",
    "            logger.info(\"Received response: %s\", response)\n",
    "            history[-1][1] += response  # type: ignore # Ensure content is properly added\n",
    "            return history\n",
    "        else:\n",
    "            logger.error(\"Not a supported model: %s\", model)\n",
    "            return \"not a supported model!\"\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in bot function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# Interface design\n",
    "def main():\n",
    "    with gr.Blocks(\n",
    "        theme=gr.themes.Base(\n",
    "            primary_hue=\"teal\",\n",
    "            secondary_hue=\"cyan\",\n",
    "            neutral_hue=\"slate\",\n",
    "        ),\n",
    "        css=css\n",
    "    ) as main_app:\n",
    "        gr.Markdown(\"# Multimodal LLM\")\n",
    "        with gr.Row():\n",
    "            ## inputs\n",
    "            with gr.Column():\n",
    "                with gr.Tab(label=\"Text Workflows\"):\n",
    "                    message = gr.Textbox(\n",
    "                        label=\"User Prompt\",\n",
    "                        placeholder=\"Place your prompt here\",\n",
    "                    )\n",
    "                with gr.Accordion(label=\"Advanced Configs\"):\n",
    "                    temperature = gr.Slider(label=\"temperature\",maximum=1, step=0.1)\n",
    "                    model = gr.Dropdown(\n",
    "                        choices=[\"gemini-pro\", \"phi3\"],\n",
    "                        value=\"gemini-pro\",\n",
    "                        label=\"Models\",\n",
    "                    )\n",
    "                    system_prompt = gr.TextArea(\n",
    "                        label=\"System Prompt:\", placeholder=\"place your System Prompt here\"\n",
    "                    )\n",
    "\n",
    "                @gr.render(inputs=model)\n",
    "                def display_api_key_input(chosen_model: str):\n",
    "                    try:\n",
    "                        if \"gemini\" in chosen_model.lower() and \"GOOGLE_API_KEY\" not in os.environ:\n",
    "                            api_key = gr.Textbox(\n",
    "                                \"\",\n",
    "                                label=\"Google API Key\",\n",
    "                                placeholder=\"Place your API key here\",\n",
    "                                type=\"password\",\n",
    "                            )\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(\"Error in display_api_key_input function: %s\", e)\n",
    "                        logger.debug(traceback.format_exc())\n",
    "                        raise\n",
    "\n",
    "            ## bot output\n",
    "            with gr.Column(elem_classes=[\"full_height\"]):\n",
    "                chatbot = gr.Chatbot(label=\"Bot Response\", height=\"100%\")\n",
    "        with gr.Row():\n",
    "            generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n",
    "            clear_btn = gr.ClearButton(variant=\"stop\")\n",
    "        with gr.Row():\n",
    "            examples = gr.Examples(\n",
    "                [\n",
    "                    \"write me the code for bubble sort in javascript\",\n",
    "                    \"write me the code for djikstra algorithm\",\n",
    "                ],\n",
    "                message,\n",
    "            )\n",
    "        gr.on(\n",
    "            [message.submit, generate_btn.click],\n",
    "            user,\n",
    "            [message, chatbot],\n",
    "            [message, chatbot],\n",
    "        ).then(\n",
    "            bot,\n",
    "            [chatbot, model, temperature, system_prompt],\n",
    "            [chatbot],\n",
    "            queue=False,\n",
    "        )\n",
    "        clear_btn.click(\n",
    "            lambda: (\"\",None), None , [message, chatbot], queue=False\n",
    "        )\n",
    "    # Driver code\n",
    "    try:\n",
    "        main_app.queue()\n",
    "        main_app.launch(debug=True)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in main function: %s\", e)\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTROLLING LAYOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[ 54, 161, 177],\n",
      "        [ 54, 161, 177],\n",
      "        [ 54, 161, 177],\n",
      "        ...,\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176]],\n",
      "\n",
      "       [[ 52, 159, 175],\n",
      "        [ 52, 159, 175],\n",
      "        [ 52, 159, 175],\n",
      "        ...,\n",
      "        [ 49, 161, 175],\n",
      "        [ 49, 161, 175],\n",
      "        [ 49, 161, 175]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]]], dtype=uint8)\n",
      "array([[[ 54, 161, 177],\n",
      "        [ 54, 161, 177],\n",
      "        [ 54, 161, 177],\n",
      "        ...,\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176],\n",
      "        [ 50, 162, 176]],\n",
      "\n",
      "       [[ 52, 159, 175],\n",
      "        [ 52, 159, 175],\n",
      "        [ 52, 159, 175],\n",
      "        ...,\n",
      "        [ 49, 161, 175],\n",
      "        [ 49, 161, 175],\n",
      "        [ 49, 161, 175]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]],\n",
      "\n",
      "       [[ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        [ 53, 160, 176],\n",
      "        ...,\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160],\n",
      "        [ 39, 135, 160]]], dtype=uint8)\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "from pprint import pprint\n",
    "\n",
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "\n",
    "def flip_image(x):\n",
    "    pprint(x)\n",
    "    return np.fliplr(x)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Flip text or image files using this demo.\")\n",
    "    with gr.Tab(\"Flip Text\"):\n",
    "        text_input = gr.Textbox()\n",
    "        text_output = gr.Textbox()\n",
    "        text_button = gr.Button(\"Flip\")\n",
    "    with gr.Tab(\"Flip Image\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image()\n",
    "            image_output = gr.Image()\n",
    "        image_button = gr.Button(\"Flip\")\n",
    "\n",
    "    with gr.Accordion(\"Open for More!\", open=False):\n",
    "        gr.Markdown(\"Look at me...\")\n",
    "        temp_slider = gr.Slider(\n",
    "            minimum=0.0,\n",
    "            maximum=1.0,\n",
    "            value=0.1,\n",
    "            step=0.1,\n",
    "            interactive=True,\n",
    "            label=\"Slide me\",\n",
    "        )\n",
    "        temp_slider.change(lambda x: x, [temp_slider])\n",
    "\n",
    "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
    "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7895\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7895/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    error_box = gr.Textbox(label=\"Error\", visible=False)\n",
    "\n",
    "    name_box = gr.Textbox(label=\"Name\")\n",
    "    age_box = gr.Number(label=\"Age\", minimum=0, maximum=100)\n",
    "    symptoms_box = gr.CheckboxGroup([\"Cough\", \"Fever\", \"Runny Nose\"])\n",
    "    submit_btn = gr.Button(\"Submit\")\n",
    "\n",
    "    with gr.Column(visible=False) as output_col:\n",
    "        diagnosis_box = gr.Textbox(label=\"Diagnosis\")\n",
    "        patient_summary_box = gr.Textbox(label=\"Patient Summary\")\n",
    "\n",
    "    def submit(name, age, symptoms):\n",
    "        if len(name) == 0:\n",
    "            return {error_box: gr.Textbox(value=\"Enter name\", visible=True)}\n",
    "        return {\n",
    "            output_col: gr.Column(visible=True),\n",
    "            diagnosis_box: \"covid\" if \"Cough\" in symptoms else \"flu\",\n",
    "            patient_summary_box: f\"{name}, {age} y/o\",\n",
    "        }\n",
    "\n",
    "    submit_btn.click(\n",
    "        submit,\n",
    "        [name_box, age_box, symptoms_box],\n",
    "        [error_box, diagnosis_box, patient_summary_box, output_col],\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7897\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7897/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "max_textboxes = 10\n",
    "\n",
    "\n",
    "def variable_outputs(k):\n",
    "    k = int(k)\n",
    "    return [gr.Textbox(visible=True)] * k + [gr.Textbox(visible=False)] * (\n",
    "        max_textboxes - k\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    slider = gr.Slider(\n",
    "        1,\n",
    "        max_textboxes,\n",
    "        value=max_textboxes,\n",
    "        step=1,\n",
    "        label=\"How many textboxes to show:\",\n",
    "    )\n",
    "    textboxes = []\n",
    "    for i in range(max_textboxes):\n",
    "        textbox = gr.Textbox(f\"Textbox {i}\")\n",
    "        textboxes.append(textbox)\n",
    "\n",
    "    slider.change(variable_outputs, slider, textboxes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7898\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7898/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_textbox = gr.Textbox()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n",
    "    input_textbox.render()\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7898\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7898/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "success = False\n",
    "def start_process(name):\n",
    "    gr.Info(\"Starting process\")\n",
    "    if name is None:\n",
    "        gr.Warning(\"Name is empty\")\n",
    "    else:\n",
    "        success = True\n",
    "    if success == False:\n",
    "        raise gr.Error(\"Process failed\")\n",
    "    return f\"hello {name}\"\n",
    "\n",
    "gr.Interface(start_process, \"text\", \"text\", theme=gr.themes.Base()).launch(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7901\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7901/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def trim_words(words, lens):\n",
    "    trimmed_words = []\n",
    "    time.sleep(5)\n",
    "    for w, l in zip(words, lens):\n",
    "        trimmed_words.append(w[: int(l)])\n",
    "    return [trimmed_words]\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        word = gr.Textbox(label=\"word\")\n",
    "        leng = gr.Textbox(label=\"leng\")\n",
    "        output = gr.Textbox(label=\"Output\")\n",
    "    with gr.Row():\n",
    "        run = gr.Button()\n",
    "\n",
    "    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
